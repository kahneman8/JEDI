import os, json, datetime
from daily_brief import fetch_news, classify_sector, analyze_sentiment, detect_themes, generate_brief
from jsonschema import validate, ValidationError

# 1. Initialize context
def run_morning_brief():
  today = datetime.date.today().strftime("%Y-%m-%d")
  print(f"[{today}] Starting morning brief generation...")
  
  # 2. Fetch latest news (global/Asia and Indonesia)
  
  try:
    global_news = fetch_news.search_and_retrieve_news("global market news Asia overnight")
    local_news = fetch_news.search_and_retrieve_news("Indonesia market news today")
    news_items = global_news + local_news
  except Exception as e:
    print(f"Error fetching news: {e}")
    news_items = [] # continue with empty list or cached data
    
  # 3. Classify each news item by GICS sector
  
  for item in news_items:
    try:
      item['sector'] = classify_sector.get_sector(item['headline'], item.get('content', ''))
    except Exception as e:
      item['sector'] = "Unknown"
      print(f"Classification error for '{item['headline']}': {e}")
      
  # 4. Analyze sentiment of each news item
  
  for item in news_items:
    try:
      item['sentiment'] = analyze_sentiment.get_sentiment(item.get('content', item['headline']))
    except Exception as e:
      item['sentiment'] = "Neutral"
      print(f"Sentiment analysis error for '{item['headline']}': {e}")
      
  # 5. Aggregate news by sector and sentiment counts
  
  sector_groups = {}
  for item in news_items:
    sector = item.get('sector', 'Unclassified')
    sector_groups.setdefault(sector, []).append(item)
    
  # Compute sentiment indicators per sector (count positives, negatives, neutrals)
  
  sentiment_indicators = {}
  for sector, items in sector_groups.items():
    sentiment_counts = {"Positive": 0, "Negative": 0, "Neutral": 0}
    for it in items:
      sentiment = it.get('sentiment', 'Neutral')
      if sentiment in sentiment_counts:
        sentiment_counts[sentiment] += 1
    sentiment_indicators[sector] = sentiment_counts
    
	# 6. Determine watchlist alerts (curated and dynamic)
	curated_alerts, dynamic_alerts = [], []
	try:
		curated_alerts = detect_themes.check_curated_watchlist(news_items)
	except Exception as e:
		print(f"Curated watchlist check error: {e}")
	try:
		dynamic_alerts = detect_themes.find_dynamic_trends(news_items)
	except Exception as e:
		print(f"Dynamic watchlist trend detection error: {e}")
	watchlist_alerts = curated_alerts + dynamic_alerts
		
	# 7. Identify emerging themes from the news

	themes = []
	try:
		themes = detect_themes.find_emerging_themes(news_items)
	except Exception as e:
		print(f"Theme detection error: {e}")
	# 8. Use LLM to generate final JSON and Markdown report
	brief_json, brief_markdown = generate_brief.compose_and_generate(
		date=today,
		market_summaries=None, # will be generated by the model based on news (optional input)
		economic_events=None, # optional: could fetch from an API or leave to model
		news_by_sector=sector_groups,
		watchlist_alerts=watchlist_alerts,
		emerging_themes=themes,
		sentiment_indicators=sentiment_indicators
	)
		
	# 9. Validate JSON output against schema
	schema_path = os.path.join(os.path.dirname(__file__), "schema.json")
	schema = json.load(open(schema_path))
	try:
		validate(instance=brief_json, schema=schema)
	except ValidationError as ve:
		print("JSON Validation failed:", ve)
	# (Optionally, attempt to fix or notify â€“ for now just log the error.)
	# 10. Save outputs to disk

	os.makedirs("outputs", exist_ok=True)
	json_filename = f"outputs/{today}_brief.json"
	md_filename = f"outputs/{today}_brief.md"
	with open(json_filename, "w") as f_json:
		json.dump(brief_json, f_json, indent=2)
	with open(md_filename, "w") as f_md:
		f_md.write(brief_markdown)
	print(f"Saved brief to {json_filename} and {md_filename}")
if __name__ == "__main__":
	run_morning_brief()

